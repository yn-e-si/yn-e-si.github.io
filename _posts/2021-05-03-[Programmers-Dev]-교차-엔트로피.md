---
title: "[Programmers Dev] 교차 엔트로피 "
excerpt: "인공지능 수학"
toc: true
toc_sticky: true
categories:
 - DL
tags:
 - 프로그래머스
 - 인공지능 수학
 - 통계학
use_math: true
---

## &#128161; 교차 엔트로피

### &#128204; 엔트로피 (Entropy)

**기본 개념**

- 자기정보 (Self - information): $i(A)$
- $A$: 사건을 의미

- $i(A) = log_b\left(\frac{1}{P(A)}\right) = -log_bP(A)$
- 확률이 높은 사건
  - 정보가 많지 않다.
  - 예) 도둑이 들었는데 개가 짖는 경우보다 도둑이 들었는데 개가 안 짖는 경우가 더 많은 정보를 포함하고 있다.

> &#128173; 확률이 높은 사건은 정보가 많지 않고 확률이 낮은 사건일수록 정보가 많이 담겨있다라는 관점에서 나온 개념이다. 이를 위해 다음의 두 가지 조건을 만족하는 로그함수로 엔트로피의 개념을 정의하였다. 1. 확률이 높으면 자기정보는 작아지고 확률이 낮아지면 자기정보는 커짐 2. 두 개의 사건 $A, B$ 가 동시에 일어났을 때, 자기정보는 $A,B$ 의 자기정보 두 개를 합친 것 과 같다.

<br/>

- 정보의 단위
  - $b = 2$ : bits
  - $b=e$ : nats
  - $b=10$ : hartleys

> &#128173; 보통 밑을 2로 많이 사용한다.

<br/>

- 특성
  - $i(AB) = log_b\left(\frac{1}{P(A)P(B)}\right) = log_b\left(\frac{1}{P(A)}\right) + log_b\left(\frac{1}{P(B)}\right) = i(A) + i(B)$

<br/>

예)

동전을 던질 때 앞면과 뒷면이 나올 확률이 다음과 같다. 이 때의 $i(H),i(T)$ 의 값은?

- $P(H) = \frac{1}{8}, P(T) = \frac{7}{8}$

<br/>

<br/>

- $i(H) = -log_2P(H) = -log_2\frac{1}{8} = 3$
- $i(T) = -log_2P(T) = -log_2\frac{7}{8} = 0.193$

> &#128173; 앞면은 3 비트, 뒷면은 0.193 비트 의 정보를 포함하고 있다 라고 해석할 수 있다. 즉 이는 어떤 사건에 대해서 그 사건이 발생할 확률을 가지고 정보의 양을 표현한 것이다.

<br/>

#### 엔트로피 

- 자기 정보의 평균이 엔트로피이다.
  - $H(X) = \sum_j P(A_j)i(A_j) = - \sum_jP(A_j)log_2P(A_j)$
- 특성
  - $0 \leq H(X) \leq log_2K$
    - $K$: 사건의 수

- 엔트로피 값이 가장 큰 경우는 모든 사건들이 동일한 확률들을 가질 때 이다.

- 엔트로피의 활용
  - 데이터를 표현할 때 필요한 평균비트수를 표현
  - 데이터 압축에 사용 가능

예)

![](https://yn-e-si.com/assets/images/program_dev/entropy_1.JPG){: .align-center}

- 위와 같이 $i(x)$ 가 주어지면 이것은 이 정보를 표현하는데 필요한 비트수를 의미하는 것이다.

- $i(x)$ 를 활용하는 경우
  - 평균비트수
    - $1 \times \frac{1}{2} + 2 \times \frac{1}{4} + 3 \times \frac{1}{8} + 3 \times \frac{1}{8}  = \frac{14}{8} = \frac{7}{4}$ 비트

> &#128173; 일반적으로 4 가지 정보를 표현하는데 평균적으로 2 비트가 필요하다 생각하지만 위의 계산 처럼 entropy 를 활용하여 평균적으로 1.75 비트만 필요하다고 구함으로써 데이터가 압축될 수 있는 것이다.

<br/>

### &#128204; 교차 엔트로피

**기본 개념**

- 확룔분포 $P$ 와 $Q$
  - $S =\{A_j\}$
    - $P(A_J)$: 확률분포 $P$ 에서 사건 $A_j$ 가 발생할 확률
    - $Q(A_j)$: 확률분포 $Q$ 에서 사건 $A_j$ 가 발생할 확률
    - $i(A_j)$: 확률분포 $Q$ 에서 사건 $A_j$ 의 자기정보
      - $i(A_j) = -log_2Q(A_j)$
      - 자기정보는 $A_j$ 를 표현하는 비트수
      - 잘못된 확률분포 $Q$ 를 사용하게 되면, 실제 최적의 비트수를 사용하지 못하게 된다.

<br/>

#### 교차 엔트로피

- 집합 $S$ 상에서 확률분포 $P$ 에 대한 확률분포 $Q$ 의 교차 엔트로피
- 확률분포 $P$ 에서 $i(A_j)$ 의 평균
  - $H(P,Q) = \sum_j P(A_j)i(A_j) = - \sum_jP(A_j) log_2Q(A_j) = - \sum_{x \in X} P(x)log_2Q(x)$
  - 이 값은 정확한 확률분포 $P$ 를 사용했을 때의 비트수보다 크게 된다.
    - $H(P,Q) = -\sum_{x \in X }P(x)log_2Q(x) \geq - \sum_{x \in X } P(x)log_2P(x) = H(P)$

- 따라서 이 값은 $P$ 와 $Q$ 가 얼마나 비슷한지를 알려주는 지표이다.
  - 같으면 $H(P,Q) = H(P)$
  - 다르면 $H(P,Q) > H(P)$

<br/>

예)

![](https://yn-e-si.com/assets/images/program_dev/entropy_2.JPG){: .align-center}

- $Q(X)$ 를 가정하고 코드 부여

  - 평균비트 수

    - $3 \times \frac{1}{2} + 3 \times \frac{1}{4} + 2 \times \frac{1}{8} + 1 \times \frac{1}{8}  = \frac{21}{8} $

    - 기존보다 1.5 배 더 많은 비트 사용이 필요하게 된다.

<br/>

&#128073; 따라서 교차 엔트로피를 사용하면 $P$ 라는 확률분포와 $Q$ 라는 확률분포가 얼마나 비슷한지를 알 수가 있다! 

<br/>

### &#128204; 분류 문제에서의 손실함수

- 분류문제
  - 주어진 대상이 A 인지 아닌지를 판단하는 것이다.
  - 즉 주어진 대상이 A, B, C, ... 중 어느 것인지를 판단하는 것이다.
- 기계학습에서는 주어진 대상이 각 그룹에 속할 확률을 제공해준다.
  - 예)
    - [0.8, 0.2] : A 일 확률 0.8, 아닐 확률 0.2
  - 이 값이 정답인 [1.0, 0.0] 과 얼마나 다른지 측정이 필요하다.

- 원하는 답: $ P = [p_1, p_2, \cdots, p_n] , p_1 +p_2+ \cdots + p_n = 1$
- 제시된 답: $Q = [q_1, q_2, \cdots, q_n], q_1 + q_2 + \cdots + q_n = 1$

- $P$ 와 $Q$ 가 얼마나 다른지에 대학 척도가 필요한데, 이를 위한 것이 손실함수이다.

> &#128173; 기계학습에서 제공한 확률과 정답과의 차이가 얼마나 나는지 측정하는 것이 손실함수이다.

<br/>

#### 손실함수

여러가지 손실함수 중 개념습득을 위해 2 가지에 대해서만 알아보자.

- 제곱합
  - 차이가 적을수록 0 에 가까워지고, 차이가 클수록 값이 커지게 된다.
  - $\sum(p_i-q_i)^2$
    - $(1.0 - 0.8)^2 + (0.0 - 0.2)^2$
  - 확률이 다를수록 큰 값을 가진다.
  - 하지만 학습 속도가 느리다는 단점이 있다.
- 교차 엔트로피 $H(P,Q)$
  - 확률이 다를수록 큰 값을 가진다.
  - 학습 속도가 빠르다는 장점이 있다.

> &#128173; 분류 문제에서는 주로 교차 엔트로피가 손실함수로 사용이 된다.

<br/>

- 참고: 분류 문제에서의 원하는 답
  - $P = [p_1, p_2, \cdots, p_n]$
    - $P_i$ 중 하나만 $1$ 이고, 나머지는 다 $0$ 임
      - entropy $= 0$ , $H(P) = 0$
    - $P_k = 1.0$ 이라고 하면, $Q_k$ 의 값이 최대한 커지는 방향으로 학습을 진행한다.

> &#128173; $H(P) = 0$ 이라는 것은 고양이를 고양이라 하는 당연한 얘기이기에 정보가 없는 것이다.

> &#128173; $P_k$ 가 $1.0$ 이라면, 원하는 답이 $k$ 인 것이기에, 다른 확률 분포 $Q_k$ 에서도 $k$ 값이 제일 높아야 분류 정확도가 높아지는 것이므로 $Q_k$ 의 값이 최대한 커지는 방향으로 학습을 진행하는 것이다.

<br/>

예)

$S = \{A, B\}$

실제정답

- $P = [1, 0]$
  - $P(A) = 1, P(B) = 0$

예측 $Q(X)$

- $[0.8, 0.2]: Q(A) = 0.8, Q(B) = 0.2$
  - $H(P,Q) = -\sum_{x \in X}P(x)log_2Q(x) = -1 \times log_20.8 = 0.3219$

<br/>

- $[0.5, 0.5]: Q(A) = 0.5, Q(B) = 0.5$
  - $H(P,Q) = -\sum_{x \in X}P(x)log_2Q(x) = -1 \times log_20.5 = 1$

<br/>

- $[0.2, 0.8]: Q(A) = 0.2, Q(B) = 0.8$
  - $H(P,Q) = -\sum_{x \in X}P(x)log_2Q(x) = -1 \times log_20.2 = 2.32$

<br/>

**python code**

```python
import numpy as np
def crossentropy(P, Q):
	return sum([-P[i]*np.log2(Q[i]) for i in range(len(P))])
P = [1, 0]

Q = [0.8, 0.2]
print(crossentropy(P, Q))

Q = [0.5, 0.5]
print(crossentropy(P, Q))

Q = [0.2, 0.8]
print(crossentropy(P,Q))
```

<br/>

<br/>

## &#128161; Check Point

- 엔트로피는 **자기 정보의 평균값**이다.
- 교차 엔트로피는 예측된 모델이 실제값을 얼마나 잘 예측해줬는지 검증해줄 수 있는 **손실함수**로써 사용된다.

